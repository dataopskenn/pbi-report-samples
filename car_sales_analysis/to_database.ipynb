{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import time\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "import pyodbc\n",
    "import yaml\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up a database connection\n",
    "\n",
    "driver = 'SQL Server'\n",
    "server = 'localhost'\n",
    "database = 'cars'\n",
    "\n",
    "cnxn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};Server='+server+';Database='+database+';Trusted_Connection=yes;')\n",
    "cursor = cnxn.cursor()\n",
    "\n",
    "engine = create_engine('mssql+pyodbc://@' + server + '/' + database + '?trusted_connection=yes&driver=ODBC+Driver+17+for+SQL+Server', fast_executemany=True)\n",
    "\n",
    "# Some other example server values are\n",
    "# server = 'localhost\\sqlexpress' # for a named instance\n",
    "# server = 'myserver,port' # to specify an alternate port\n",
    "# driver = 'ODBC Driver 17 for SQL Server' \n",
    "\n",
    "# username = 'myusername' \n",
    "# password = 'mypassword' \n",
    "# ENCRYPT defaults to yes starting in ODBC Driver 18. It's good to always specify ENCRYPT=yes on the client side to avoid MITM attacks.\n",
    "# cnxn = pyodbc.connect('DRIVER={ODBC Driver 18 for SQL Server};SERVER='+server+';DATABASE='+database+';ENCRYPT=yes;UID='+username+';PWD='+ password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database_connection():\n",
    "\n",
    "    \"\"\"\n",
    "    Establish connection to the database \n",
    "    Return the connection and cursor refrence\n",
    "    :return: returns (cur, conn) a cursor and connection reference\n",
    "    \"\"\"\n",
    "\n",
    "    # Opening the YAML config file\n",
    "    with open(\"mssql_config.yml\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # from config.yml import user name and password\n",
    "    config = yaml.load(content, Loader=yaml.FullLoader)\n",
    "    \n",
    "    cnxn = pyodbc.connect(\n",
    "        'DRIVER={ODBC Driver 17 for SQL Server};SERVER='+server+';DATABASE='+database+';Trusted_Connection=yes', fast_executemany=True, pool_pre_ping=True\n",
    "        )\n",
    "    cur = cnxn.cursor()\n",
    "    \n",
    "    return cur, cnxn\n",
    "\n",
    "\n",
    "def drop_tables(cur, cnxn):\n",
    "\n",
    "    # cur.execute(\"\"\"CREATE SCHEMA cars \"\"\")\n",
    "\n",
    "    cur.execute(\"\"\" DROP TABLE IF EXISTS cars.bodytype\"\"\")\n",
    "    cur.execute(\"\"\" DROP TABLE IF EXISTS cars.categories\"\"\")\n",
    "    cur.execute(\"\"\" DROP TABLE IF EXISTS cars.condition\"\"\")\n",
    "    cur.execute(\"\"\" DROP TABLE IF EXISTS cars.listing\"\"\")\n",
    "    cur.execute(\"\"\" DROP TABLE IF EXISTS cars.trueprices\"\"\")\n",
    "    cnxn.commit()\n",
    "    print(\"Successfully dropped tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r\"Datasets\"\n",
    "\n",
    "def process_data(filepath):\n",
    "\n",
    "    \"\"\"\n",
    "    import the datasets from where they are (locally or on the web) directly to the database\n",
    "    - In this case, the datasets are sitting locally in CSV files on my machine\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # get all files matching extension from directory\n",
    "    for root, dirs, files in os.walk(filepath):\n",
    "        files = glob.glob(os.path.join(root + '/*.csv'))\n",
    "\n",
    "        for file in files:\n",
    "            df = pd.read_csv(file, header = 0, delimiter=\";\", decimal = \",\", index_col=False)\n",
    "\n",
    "            df.to_sql(f'{file[9:-4]}', engine, schema='cars', if_exists='append', index=False, chunksize=1000000)\n",
    "            print(f\"data transformed and inserted for {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def database_main():\n",
    "\n",
    "    cur, cnxn = create_database_connection()\n",
    "    drop_tables(cur, cnxn)\n",
    "    cnxn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully dropped tables\n",
      "data transformed and inserted for Datasets\\bodytype.csv\n",
      "data transformed and inserted for Datasets\\categories.csv\n",
      "data transformed and inserted for Datasets\\condition.csv\n",
      "data transformed and inserted for Datasets\\listing.csv\n",
      "data transformed and inserted for Datasets\\trueprices.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    database_main()\n",
    "    process_data(filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
